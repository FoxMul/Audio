# Audio
!pip install webrtcvad
!pip install noisereduce
import tensorflow as tf
import tempfile
import sys
import subprocess
import tempfile
import noisereduce as nr
import wave
import pickle
from pathlib import Path
import contextlib
import librosa
import numpy as np
import IPython.display as ipd
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.mixture import GaussianMixture
from scipy.spatial.distance import cdist
from scipy.io import wavfile
import webrtcvad
import collections
import copy
import os
from IPython.display import clear_output
from sklearn.cluster import SpectralClustering
import warnings

def _stft(y, n_fft, hop_length, win_length, use_tensorflow=False):
    if use_tensorflow:
        return _stft_tensorflow(y, n_fft, hop_length, win_length)
    else:
        return librosa.stft(
            y=y, n_fft=n_fft, hop_length=hop_length, win_length=win_length, center=True)


def _istft(y, n_fft, hop_length, win_length, use_tensorflow=False):
    if use_tensorflow:
        return _istft_tensorflow(y.T, n_fft, hop_length, win_length)
    else:
        return librosa.istft(y, hop_length, win_length)


def _stft_librosa(y, n_fft, hop_length, win_length):
    return librosa.stft(
        y=y, n_fft=n_fft, hop_length=hop_length, win_length=win_length, center=True)


def _istft_librosa(y, hop_length, win_length):
    return librosa.istft(y, hop_length, win_length)


def _stft_tensorflow(y, n_fft, hop_length, win_length):
    return (tf.signal.stft( y,win_length,hop_length,n_fft,pad_end=True,window_fn=tf.signal.hann_window).numpy().T)


def _istft_tensorflow(y, n_fft, hop_length, win_length):
    return tf.signal.inverse_stft(
        y.astype(np.complex64), win_length, hop_length, n_fft).numpy()


def _amp_to_db(x):
    return librosa.core.amplitude_to_db(x, ref=1.0, amin=1e-20, top_db=80.0)


def _db_to_amp(x,):
    return librosa.core.db_to_amplitude(x, ref=1.0)


def update_pbar(pbar, message):
   if pbar is not None:
        pbar.set_description(message)
        pbar.update(1)


def _smoothing_filter(n_grad_freq, n_grad_time):
   
    smoothing_filter = np.outer( np.concatenate([np.linspace(0, 1, n_grad_freq + 1, endpoint=False), np.linspace(1, 0, n_grad_freq + 2),])[1:-1],np.concatenate(
            [np.linspace(0, 1, n_grad_time + 1, endpoint=False),np.linspace(1, 0, n_grad_time + 2),])[1:-1],)
    smoothing_filter = smoothing_filter / np.sum(smoothing_filter)
    return smoothing_filter


def mask_signal(sig_stft, sig_mask):
    sig_stft_amp = sig_stft * (1 - sig_mask)
    return sig_stft_amp


def convolve_gaussian(sig_mask, smoothing_filter, use_tensorflow=False):
   if use_tensorflow: 
      smoothing_filter = smoothing_filter * ((np.shape(smoothing_filter)[1] - 1) / 2 + 1)
      smoothing_filter = smoothing_filter[:, :, tf.newaxis, tf.newaxis].astype()
      img = sig_mask[:, :, tf.newaxis, tf.newaxis].astype("float32")
      return (tf.nn.conv2d(img, smoothing_filter, strides=[1, 1, 1, 1], padding="SAME").numpy().squeeze())
   else:
       return scipy.signal.fftconvolve(sig_mask, smoothing_filter, mode="same")

def reduce_noise(
    audio_clip,
    noise_clip,
    n_grad_freq=2,
    n_grad_time=4,
    n_fft=2048,
    win_length=2048,
    hop_length=512,
    n_std_thresh=1.5,
    prop_decrease=1.0,
    pad_clipping=True,
    use_tensorflow=False,
    verbose=False):
   
    if use_tensorflow:
        use_tensorflow = load_tensorflow(verbose)

    if verbose:
        pbar = tqdm(total=7)
    else:
        pbar = None

    update_pbar(pbar, "STFT on noise")
    noise_stft = _stft(noise_clip, n_fft, hop_length, win_length, use_tensorflow=use_tensorflow)
    noise_stft_db = _amp_to_db(np.abs(noise_stft)) 
    update_pbar(pbar, "STFT on signal")
    mean_freq_noise = np.mean(noise_stft_db, axis=1)
    std_freq_noise = np.std(noise_stft_db, axis=1)
    noise_thresh = mean_freq_noise + std_freq_noise * n_std_thresh
    update_pbar(pbar, "STFT on signal")

    if pad_clipping:
        nsamp = len(audio_clip)
        audio_clip = np.pad(audio_clip, [0, hop_length], mode="constant")

    sig_stft = _stft(audio_clip, n_fft, hop_length, win_length, use_tensorflow=use_tensorflow)
    sig_stft_db = _amp_to_db(np.abs(sig_stft))
    update_pbar(pbar, "Generate mask")

    db_thresh = np.repeat(np.reshape(noise_thresh, [1, len(mean_freq_noise)]),np.shape(sig_stft_db)[1],axis=0,).T
    sig_mask = sig_stft_db < db_thresh
    update_pbar(pbar, "Smooth mask")
    smoothing_filter = _smoothing_filter(n_grad_freq, n_grad_time)
    sig_mask = convolve_gaussian(sig_mask, smoothing_filter, use_tensorflow)
    sig_mask = sig_mask * prop_decrease
    update_pbar(pbar, "Apply mask")

    sig_stft_amp = mask_signal(sig_stft, sig_mask)

    update_pbar(pbar, "Recover signal")
    recovered_signal = _istft(
        sig_stft_amp, n_fft, hop_length, win_length, use_tensorflow=use_tensorflow)
    if pad_clipping:
        recovered_signal = librosa.util.fix_length(recovered_signal, nsamp)

    recovered_spec = _amp_to_db(np.abs(_stft(recovered_signal,n_fft,hop_length,win_length,use_tensorflow=use_tensorflow,)))
    if verbose:
        plot_reduction_steps(noise_stft_db,mean_freq_noise,std_freq_noise,noise_thresh,smoothing_filter,sig_stft_db,sig_mask,recovered_spec)
    return recovered_signal
    
    def extract_features(y, sr, window, hop, n_mfcc):
    mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=int(hop*sr), n_fft=int(window*sr), n_mfcc=n_mfcc, dct_type=2)
    mfcc_delta = librosa.feature.delta(mfcc)
    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)
    stacked = np.vstack((mfcc, mfcc_delta, mfcc_delta2))
    return stacked.T

def write_wave(path, audio, sample_rate):
    with contextlib.closing(wave.open(path, 'wb')) as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio)

class Frame(object):
    def __init__(self, bytes, timestamp, duration):
        self.bytes = bytes
        self.timestamp = timestamp
        self.duration = duration


def frame_generator(frame_duration_ms, audio, sample_rate):
    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)
    offset = 0
    timestamp = 0.0
    duration = (float(n) / sample_rate) / 2.0
    while offset + n < len(audio):
        yield Frame(audio[offset:offset + n], timestamp, duration)
        timestamp += duration
        offset += n

def vad_collector(sample_rate, frame_duration_ms, padding_duration_ms, vad, frames):
    num_padding_frames = int(padding_duration_ms / frame_duration_ms)
    ring_buffer = collections.deque(maxlen=num_padding_frames)
    triggered = False

    voiced_frames = []
    for frame in frames:
        is_speech = vad.is_speech(frame.bytes, sample_rate)

        if not triggered:
            ring_buffer.append((frame, is_speech))
            num_voiced = len([f for f, speech in ring_buffer if speech])
            if num_voiced > 0.9 * ring_buffer.maxlen:
                triggered = True
                for f, s in ring_buffer:
                    voiced_frames.append(f)
                ring_buffer.clear()
        else:
            voiced_frames.append(frame)
            ring_buffer.append((frame, is_speech))
            num_unvoiced = len([f for f, speech in ring_buffer if not speech])
            if num_unvoiced > 0.9 * ring_buffer.maxlen:
                triggered = False
                yield b''.join([f.bytes for f in voiced_frames])
                ring_buffer.clear()
                voiced_frames = []
    if voiced_frames:
        yield b''.join([f.bytes for f in voiced_frames])
        
        def map_adaptation(gmm, data, max_iterations = 300, likelihood_threshold = 1e-20, relevance_factor = 16):
    N = data.shape[0]
    D = data.shape[1]
    K = gmm.n_components
    
    mu_new = np.zeros((K,D))
    n_k = np.zeros((K,1))
    
    mu_k = gmm.means_
    cov_k = gmm.covariances_
    pi_k = gmm.weights_

    old_likelihood = gmm.score(data)
    new_likelihood = 0
    iterations = 0
    while(abs(old_likelihood - new_likelihood) > likelihood_threshold and iterations < max_iterations):
        iterations += 1
        old_likelihood = new_likelihood
        z_n_k = gmm.predict_proba(data)
        n_k = np.sum(z_n_k,axis = 0)

        for i in range(K):
            temp = np.zeros((1,D))
            for n in range(N):
                temp += z_n_k[n][i]*data[n,:]
            mu_new[i] = (1/n_k[i])*temp

        adaptation_coefficient = n_k/(n_k + relevance_factor)
        for k in range(K):
            mu_k[k] = (adaptation_coefficient[k] * mu_new[k]) + ((1 - adaptation_coefficient[k]) * mu_k[k])
        gmm.means_ = mu_k

        log_likelihood = gmm.score(data)
        new_likelihood = log_likelihood
        print(log_likelihood)
    return gmm
    
    SR = 8000 
N_MFCC = 13 
N_FFT = 0.032  
HOP_LENGTH = 0.010

N_COMPONENTS = 16 
COVARINACE_TYPE = 'full'

y=[]
LOAD_SIGNAL = True
if LOAD_SIGNAL:
    y, sr = librosa.load(' Файл  ', sr=SR)
    pre_emphasis = 0.97
    y = np.append(y[0], y[1:] - pre_emphasis * y[:-1])
    
    MAKE_CHUNKS = True
if MAKE_CHUNKS:
    vad = webrtcvad.Vad(2)
    audio = np.int16(y/np.max(np.abs(y)) * 32768)

    frames = frame_generator(10, audio, sr)
    frames = list(frames)
    segments = vad_collector(sr, 50, 200, vad, frames)

    try:
          os.makedirs("chunks") 
    except FileExistsError:
        pass

    for i, segment in enumerate(segments):
        chunk_name = 'chunks/chunk-%003d.wav' % (i,) 
        write_wave(chunk_name, segment[0: len(segment)-int(100*sr/1000)], sr)
        
FEATURES_FROM_FILE = True

feature_file_name = 'features_{0}.pkl'.format(N_MFCC)

if FEATURES_FROM_FILE:
    ubm_features=pickle.load(open(feature_file_name, 'rb'))
else:
    ubm_features = extract_features(np.array(y), sr, window=N_FFT, hop=HOP_LENGTH, n_mfcc=N_MFCC)
    ubm_features = preprocessing.scale(ubm_features)
    pickle.dump(ubm_features, open(feature_file_name, "wb")) 
    
UBM_FROM_FILE = True

ubm_file_name = 'ubm_{0}_{1}_{2}MFCC.pkl'.format(N_COMPONENTS, COVARINACE_TYPE, N_MFCC)

if UBM_FROM_FILE:
    ubm=pickle.load(open(ubm_file_name, 'rb'))
else:
    ubm = GaussianMixture(n_components = N_COMPONENTS, covariance_type = COVARINACE_TYPE)
    ubm.fit(ubm_features)
    pickle.dump(ubm, open(ubm_file_name, "wb"))
    
print(ubm.score(ubm_features))

SV = []

for i in range(100):
    clear_output(wait=True)
    fname = 'chunks/chunk-%003d.wav' % (i,)
    print('UBM MAP adaptation for {0}'.format(fname))
    y_, sr_ = librosa.load(fname, sr=None)
    f_ = extract_features(y_, sr_, window=N_FFT, hop=HOP_LENGTH, n_mfcc=N_MFCC)
    f_ = preprocessing.scale(f_)
    gmm = copy.deepcopy(ubm)
    gmm = map_adaptation(gmm, f_, max_iterations=1, relevance_factor=16)
    sv = gmm.means_.flatten()
    sv = preprocessing.scale(sv)
    SV.append(sv)

SV = np.array(SV)
clear_output()
print(SV.shape)

N_CLUSTERS = 2

def rearrange(labels, n):
    seen = set()
    distinct = [x for x in labels if x not in seen and not seen.add(x)]
    correct = [i for i in range(n)]
    dict_ = dict(zip(distinct, correct))
    return [x if x not in dict_ else dict_[x] for x in labels]

sc = SpectralClustering(n_clusters=N_CLUSTERS, affinity='cosine')
labels = sc.fit_predict(SV)
labels = rearrange(labels, N_CLUSTERS)
print(labels)

print([i for i, x in enumerate(labels) if x == 1])
